[base]
path = {base}
network = hadoop

[java]
command = share_java
alias = java
restart = false

[sbt]
alias = sbt

[mysql]
port = 3309:3306
volumes = {container}conf/mysql:/etc/mysql,/mysql10/{name}/data:/var/lib/mysql
environment = MYSQL_ROOT_PASSWORD=123456

[hive]
volumes = {container}conf/java/hive:/usr/local/hive/conf
alias = hive

[zookeeper]
num = 3
volumes = {container}conf/java/zookeeper:/usr/local/zookeeper/conf
command = zookeeper {i}
alias = zkCli.sh -server data-zookeeper:2181->zkcli

[spark]
volumes = {container}conf/java/spark:/usr/local/spark/conf
command = spark_log share
alias = spark-shell,pyspark,spark-submit --class org.apache.spark.examples.SparkPi --driver-memory 512M --executor-memory 512M --master yarn --deploy-mode client /usr/local/spark/examples/jars/spark-examples_2.11-2.1.1.jar->spark-pi,spark-submit --driver-memory 512M --executor-memory 512M --master yarn --deploy-mode client /share/src/spark/target/scala-2.11/*.jar --class ->spark-submit

[hadoop]
port = 50070:50070,8088:8088
volumes = {container}conf/java/hadoop:/usr/local/hadoop/etc/hadoop
command = hadoop
alias = hadoop
slave = 2
hook.start = hadoop

[hbase]
port = 60010:60010
volumes = {container}conf/java/hbase:/usr/local/hbase/conf
command = hbase
slave = 2
alias = hbase

[zeppelin]
port = 9999:6060
volumes = {container}conf/java/zeppelin:/usr/local/zeppelin/conf
command = zeppelin

[#master]
num = 1
port = 50070:50070,8088:8088,9000:9000,50090:50090,6066:6066,7077:7077,8080:8080,8081:8081,16010:16010
#volumes = {container}conf/data/hadoop:/usr/local/hadoop/etc/hadoop,{container}conf/data/spark:/usr/local/spark/conf,{container}conf/data/zookeeper:/usr/local/zookeeper/conf,{container}conf/data/hbase:/usr/local/hbase/conf,{container}conf/data/flume:/usr/local/flume/conf,{container}logs/nginx/web-nginx/logs:/root/flume/output,{container}conf/data/kafka:/usr/local/kafka/conf,{container}src/java/lib:/opt/jdk
#command = zookeeper-0 hadoop hbase
#command = zookeeper-0 hadoop flume kafka
command = hadoop